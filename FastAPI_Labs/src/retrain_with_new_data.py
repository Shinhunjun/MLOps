#!/usr/bin/env python3
"""
Script to retrain models using new data
"""
import os
import sys
import json
import glob
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import models
import time

def load_new_data():
    """Load new data from new_data/sub_set_N folder"""
    # Check current sub_set from count.json
    count_file = "new_data/count.json"
    if not os.path.exists(count_file):
        print("âŒ count.json file does not exist.")
        return None, None
    
    with open(count_file, 'r') as f:
        count_data = json.load(f)
    
    # Load data from previous sub_set (current - 1)
    sub_set_count = count_data['sub_set_count']
    if sub_set_count == 0:
        print("âŒ No data available for training yet.")
        return None, None
    
    sub_set_dir = f"new_data/sub_set_{sub_set_count - 1}"
    if not os.path.exists(sub_set_dir):
        print(f"âŒ {sub_set_dir} folder does not exist.")
        return None, None
    
    if not os.listdir(sub_set_dir):
        print(f"âŒ {sub_set_dir} folder is empty.")
        return None, None
    
    print(f"ğŸ“ Loading data from {sub_set_dir} folder.")
    return load_data_from_directory(sub_set_dir)

def load_data_from_directory(data_dir):
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì°¾ê¸°
    metadata_files = glob.glob(os.path.join(data_dir, "metadata.json"))
    if not metadata_files:
        print("âŒ metadata.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    with open(metadata_files[0], 'r') as f:
        metadata = json.load(f)
    
    X_new = []
    y_new = []
    
    # ê° ë°ì´í„° í¬ì¸íŠ¸ ì²˜ë¦¬
    for item in metadata:
        image_path = os.path.join(data_dir, item['filename'])
        if os.path.exists(image_path):
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
                image = Image.open(image_path).convert('L')
                image = image.resize((28, 28))
                pixels = np.array(image).flatten() / 255.0
                
                X_new.append(pixels)
                y_new.append(item['true_label'])
                
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {image_path} - {e}")
                continue
    
    if len(X_new) == 0:
        print("âŒ ìœ íš¨í•œ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    print(f"âœ… {len(X_new)}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return np.array(X_new), np.array(y_new)

def load_original_data():
    """ê¸°ì¡´ MNIST ë°ì´í„° ë¡œë“œ"""
    from data import load_data
    X, y = load_data()
    return X, y


def retrain_cnn(X_original, y_original, X_new, y_new):
    """CNN ëª¨ë¸ ì¬í›ˆë ¨"""
    print("ğŸ”„ CNN ëª¨ë¸ ì¬í›ˆë ¨ ì¤‘...")
    
    # ìƒˆë¡œìš´ ë°ì´í„°ì— ì¦ê°• ì ìš©
    print(f"ğŸ”„ ìƒˆë¡œìš´ ë°ì´í„° ì¦ê°• ì¤‘... (ì¦ê°• ë°°ìˆ˜: 3)")
    try:
        from augmentation import augment_dataset
        X_new_augmented, y_new_augmented = augment_dataset(X_new, y_new, 3)
        print(f"âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ: {len(X_new)}ê°œ â†’ {len(X_new_augmented)}ê°œ")
    except ImportError:
        print("âš ï¸ augmentation ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦ê°• ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
        X_new_augmented, y_new_augmented = X_new, y_new
    except Exception as e:
        print(f"âš ï¸ ë°ì´í„° ì¦ê°• ì‹¤íŒ¨: {e}. ì›ë³¸ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        X_new_augmented, y_new_augmented = X_new, y_new
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ì¦ê°•ëœ ìƒˆë¡œìš´ ë°ì´í„° ê²°í•©
    X_combined = np.vstack([X_original, X_new_augmented])
    y_combined = np.hstack([y_original, y_new_augmented])
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X_combined, y_combined, test_size=0.2, random_state=42
    )
    
    # í›ˆë ¨ ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë‹¤ì‹œ ë¶„í• 
    X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    # CNN ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ëª¨ë¸)
    try:
        cnn_model = models.load_model("model/cnn_mnist_model.h5")
        print("âœ… ê¸°ì¡´ CNN ëª¨ë¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except:
        print("âš ï¸ ê¸°ì¡´ CNN ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        from cnn_model import create_cnn_model
        cnn_model = create_cnn_model()
    
    # ë°ì´í„° í˜•íƒœ ë³€í™˜ (28, 28, 1)
    X_train_cnn = X_train_cnn.reshape(-1, 28, 28, 1)
    X_val_cnn = X_val_cnn.reshape(-1, 28, 28, 1)
    X_test_cnn = X_test.reshape(-1, 28, 28, 1)
    
    # ëª¨ë¸ ì¬í›ˆë ¨ (fine-tuning)
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    
    callbacks = [
        EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)
    ]
    
    start_time = time.time()
    history = cnn_model.fit(
        X_train_cnn, y_train_cnn,
        validation_data=(X_val_cnn, y_val_cnn),
        epochs=10,  # ì ì€ ì—í¬í¬ë¡œ fine-tuning
        batch_size=64,
        callbacks=callbacks,
        verbose=1
    )
    training_time = time.time() - start_time
    
    # ì •í™•ë„ í‰ê°€
    test_loss, test_accuracy = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)
    print(f"âœ… CNN ì¬í›ˆë ¨ ì™„ë£Œ! ì •í™•ë„: {test_accuracy:.4f}, ì‹œê°„: {training_time:.2f}ì´ˆ")
    
    # ëª¨ë¸ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
    timestamp = int(time.time())
    model_filename = f"model/cnn_mnist_model_{timestamp}.h5"
    cnn_model.save(model_filename)
    print(f"ğŸ’¾ CNN ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_filename}")
    
    # ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ë“¤ ì •ë¦¬ (ìµœì‹  3ê°œë§Œ ìœ ì§€)
    cleanup_old_models(timestamp)
    
    return cnn_model

def cleanup_old_models(current_timestamp):
    """ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ë“¤ì„ ì •ë¦¬í•˜ì—¬ ìµœì‹  3ê°œë§Œ ìœ ì§€"""
    import glob
    import os
    
    model_dir = "model"
    if not os.path.exists(model_dir):
        return
    
    # cnn_mnist_model_*.h5 íŒ¨í„´ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
    model_files = glob.glob(os.path.join(model_dir, "cnn_mnist_model_*.h5"))
    
    if len(model_files) <= 3:
        print("ğŸ“ ëª¨ë¸ íŒŒì¼ ê°œìˆ˜ê°€ 3ê°œ ì´í•˜ì…ë‹ˆë‹¤. ì •ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
    model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))
    
    # ì˜¤ë˜ëœ íŒŒì¼ë“¤ ì‚­ì œ (ìµœì‹  3ê°œ ì œì™¸)
    files_to_delete = model_files[:-3]
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ëª¨ë¸ íŒŒì¼ ì‚­ì œ: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file_path}: {e}")
    
    print(f"ğŸ“ ëª¨ë¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ. í˜„ì¬ {len(model_files) - len(files_to_delete)}ê°œ íŒŒì¼ ìœ ì§€")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ëª¨ë¸ ì¬í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ìƒˆë¡œìš´ ë°ì´í„° ë¡œë“œ
    X_new, y_new = load_new_data()
    if X_new is None:
        print("âŒ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    print("ğŸ“Š ê¸°ì¡´ MNIST ë°ì´í„°ë¥¼ ë¡œë“œ ì¤‘...")
    X_original, y_original = load_original_data()
    
    print(f"ğŸ“ˆ ë°ì´í„° í†µê³„:")
    print(f"  - ê¸°ì¡´ ë°ì´í„°: {len(X_original)}ê°œ")
    print(f"  - ìƒˆë¡œìš´ ë°ì´í„°: {len(X_new)}ê°œ")
    print(f"  - ì´ ë°ì´í„°: {len(X_original) + len(X_new)}ê°œ")
    
    # ëª¨ë¸ ì¬í›ˆë ¨
    try:
        # CNN ì¬í›ˆë ¨
        retrain_cnn(X_original, y_original, X_new, y_new)
        
        # ì‚¬ìš©ëœ ë°ì´í„°ëŠ” sub_set_N êµ¬ì¡°ì—ì„œ ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨
        # archive_used_data()  # ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ
        
        print("ğŸ‰ CNN ëª¨ë¸ ì¬í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ ì¬í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
